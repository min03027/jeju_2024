import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss
from datetime import datetime
import streamlit as st
import google.generativeai as genai

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

genai.configure(api_key="AIzaSyAsX-SMGt5XlHc6i8TATucxPX3qCDbVyJI")
model = genai.GenerativeModel("gemini-1.5-flash")

# CSV íŒŒì¼ ë¡œë“œ
csv_file_path = "JEJU_MCT_DATA_modified.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)

# Streamlit App UI
st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘")
    st.subheader("ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?")
    time = st.sidebar.selectbox("", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")
    opening_date_condition = st.sidebar.selectbox("", ["ì˜¤ë˜ëœ ë§›ì§‘", "ìš”ì¦˜ ëœ¨ëŠ” ê³³"], key="Opening_date", label_visibility="hidden")
    local_choice = st.radio('', ('ì œì£¼ë„ë¯¼ ë§›ì§‘', 'ê´€ê´‘ê° ë§›ì§‘'))

# íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")
st.write("#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤")

# ì´ë¯¸ì§€ ì¶œë ¥
image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
st.markdown(f"<div style='display: flex; justify-content: center;'><img src='{image_path}' alt='centered image' width='50%'></div>", unsafe_allow_html=True)

# ì±„íŒ… ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text):
    if not text:  # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ
        raise ValueError("Input text cannot be empty.")
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=3):
    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    index = load_faiss_index(index_path)

    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question).reshape(1, -1)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    distances, indices = index.search(query_embedding, k*3)
    filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True)

    # ì˜ì—…ì‹œê°„ í•„í„°ë§
    if time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

    # ê°œì„¤ì¼ì í•„í„°ë§
    current_year = datetime.now().year

    if opening_date_condition == "ì˜¤ë˜ëœ ë§›ì§‘":
        filtered_df = filtered_df[filtered_df['ê°€ë§¹ì ê°œì„¤ì¼ì'].apply(lambda x: 2024 - int(str(x)[:4]) >= 20)]
    elif opening_date_condition == "ìš”ì¦˜ ëœ¨ëŠ” ê³³":
        filtered_df = filtered_df[filtered_df['ê°€ë§¹ì ê°œì„¤ì¼ì'].apply(lambda x: 2024 - int(str(x)[:4]) <= 5)]

    # í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì²˜ë¦¬
    if filtered_df.empty:
        return f"ì„ íƒí•˜ì‹  ì¡°ê±´({opening_date_condition})ì— ë§ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.reset_index(drop=True).head(k)

    # í˜„ì§€ì¸ ë§›ì§‘ ì˜µì…˜
    if local_choice == 'ì œì£¼ë„ë¯¼ ë§›ì§‘':
        local_choice = 'ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘'
    elif local_choice == 'ê´€ê´‘ê° ë§›ì§‘':
        local_choice = 'í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘'

    # ì‘ë‹µ ìƒì„±
    reference_info = "\n".join(filtered_df['text'].tolist())
    prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"

    response = model.generate_content(prompt)
    return response.text if hasattr(response, 'text') else str(response)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# ì‘ë‹µ ìƒì„±
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)
            st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
