import os
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
import faiss
from datetime import datetime
import streamlit as st
import google.generativeai as genai
import random

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

genai.configure(api_key="AIzaSyAsX-SMGt5XlHc6i8TATucxPX3qCDbVyJI")
model = genai.GenerativeModel("gemini-1.5-flash")

# CSV íŒŒì¼ ë¡œë“œ
csv_file_path = "data/JEJU_DATA.csv"
df = pd.read_csv(csv_file_path, encoding='cp949')
#xlsx_file_path = "data/ì œì£¼_ê´€ê´‘ìˆ˜ìš”_ì˜ˆì¸¡_ë°ì´í„°merged1.xlsx"
#visit_df= pd.read_excel(xlsx_file_path)

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)

# Streamlit App UI
st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘")
    st.subheader("ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?")
    time = st.sidebar.selectbox("", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")
    opening_date_condition = st.sidebar.selectbox("", ["ì˜¤ë˜ëœ ë§›ì§‘", "ìš”ì¦˜ ëœ¨ëŠ” ê³³"], key="Opening_date", label_visibility="hidden")
    month = st.sidebar.selectbox("", ["1ì›”", "2ì›”", "3ì›”", "4ì›”", "5ì›”","6ì›”", "7ì›”", "8ì›”", "9ì›”", "10ì›”", "11ì›”", "12ì›”"], key="month_data")
    local_choice = st.radio('', ('ì œì£¼ë„ë¯¼ ë§›ì§‘', 'ê´€ê´‘ê° ë§›ì§‘'))

# íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")
st.write("#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤")

# ì´ë¯¸ì§€ ì¶œë ¥
image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
st.markdown(f"<div style='display: flex; justify-content: center;'><img src='{image_path}' alt='centered image' width='50%'></div>", unsafe_allow_html=True)

# ì±„íŒ… ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    if os.path.exists(index_path):
        index = faiss.read_index(index_path)
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text):
    if not text:
        raise ValueError("Input text cannot be empty.")
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))


# ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=3):
    index = load_faiss_index(index_path)

    query_embedding = embed_text(question).reshape(1, -1)
    distances, indices = index.search(query_embedding, k*3)
    filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True)

    if time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

    current_year = datetime.now().year
    if opening_date_condition == "ì˜¤ë˜ëœ ë§›ì§‘":
        filtered_df = filtered_df[filtered_df['ê°€ë§¹ì ê°œì„¤ì¼ì'].apply(lambda x: 2024 - int(str(x)[:4]) >= 10)]
    elif opening_date_condition == "ìš”ì¦˜ ëœ¨ëŠ” ê³³":
        filtered_df = filtered_df[filtered_df['ê°€ë§¹ì ê°œì„¤ì¼ì'].apply(lambda x: 2024 - int(str(x)[:4]) <= 5)]

    #if month =='1ì›”':
        #filtered_df = 





    # í•„í„°ë§ëœ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì²˜ë¦¬
    if filtered_df.empty:
        # ë‹¤ë¥¸ ë§›ì§‘ ì¶”ì²œ
        return f"ì„ íƒí•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.reset_index(drop=True).head(k)
    
    if local_choice == 'ì œì£¼ë„ë¯¼ ë§›ì§‘':
        local_choice = 'ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘'
    elif local_choice == 'ê´€ê´‘ê° ë§›ì§‘':
        local_choice = 'í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘'

    reference_info = "\n".join(filtered_df['text'].tolist())
    prompt = f"ì§ˆë¬¸: {question} íŠ¹íˆ {local_choice}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"

    response = model.generate_content(prompt)
    return response.text if hasattr(response, 'text') else str(response)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# ì‘ë‹µ ìƒì„±
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)
            st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
