import os
import json
import uuid
import numpy as np
import pandas as pd
from datetime import datetime
from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import streamlit as st
import google.generativeai as genai

# **1. ë³´ì•ˆ ì„¤ì •: API í‚¤ ê´€ë¦¬**
# **ì¤‘ìš”:** API í‚¤ë¥¼ ì½”ë“œì— ì§ì ‘ í¬í•¨ì‹œí‚¤ì§€ ë§ê³ , í™˜ê²½ ë³€ìˆ˜ë‚˜ Streamlit Secretsë¥¼ í†µí•´ ê´€ë¦¬í•˜ì„¸ìš”.
# ì—¬ê¸°ì„œëŠ” Streamlit Secretsë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.
# .streamlit/secrets.toml íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ API í‚¤ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
# GOOGLE_API_KEY = "YOUR_ACTUAL_API_KEY"

try:
    # GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key="AIzaSyAsX-SMGt5XlHc6i8TATucxPX3qCDbVyJI")
    model = genai.GenerativeModel("gemini-1.5-flash")
except KeyError:
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()
except Exception as e:
    st.error(f"Gemini ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# **2. Streamlit í˜ì´ì§€ ì„¤ì •**
st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë ˆìŠ¤í† ë‘!", layout="wide")
st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")

# **3. ì´ë¯¸ì§€ í‘œì‹œ**
image_url = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_url}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

# **4. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**
data_path = './data'
csv_file_path = "JEJU_DATA.csv"

def load_csv(file_path):
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path, encoding='cp949')
            df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)
            return df
        except Exception as e:
            st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return pd.DataFrame()
    else:
        st.error(f"{file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

df = load_csv(os.path.join(data_path, csv_file_path))

# **5. FAISS ë° ì„ë² ë”© ì„¤ì •**
module_path = './modules'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

try:
    tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
    embedding_model = AutoModel.from_pretrained("jhgan/ko-sroberta-multitask").to(device)
except Exception as e:
    st.error(f"í† í¬ë‚˜ì´ì € ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

def load_embeddings(file_path):
    if os.path.exists(file_path):
        try:
            return np.load(file_path)
        except Exception as e:
            st.error(f"ì„ë² ë”© íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    else:
        st.error(f"{file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

embeddings = load_embeddings(os.path.join(module_path, 'embeddings_array_file.npy'))

def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    if os.path.exists(index_path):
        try:
            return faiss.read_index(index_path)
        except Exception as e:
            st.error(f"FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    else:
        st.error(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

faiss_index = load_faiss_index()

# **6. í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜**
def embed_text(text):
    try:
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
        with torch.no_grad():
            embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
        return embeddings.squeeze().cpu().numpy()
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# **7. ì‘ë‹µ ìƒì„± í•¨ìˆ˜**
def generate_response_with_faiss(question, df, embeddings, faiss_index, model, embed_text, k=3):
    if embeddings is None or faiss_index is None:
        return "ì„ë² ë”© íŒŒì¼ ë˜ëŠ” FAISS ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        query_embedding = embed_text(question).reshape(1, -1)
        distances, indices = faiss_index.search(query_embedding, k * 3)
        filtered_df = df.iloc[indices[0, :]].copy().reset_index(drop=True).head(k)

        if filtered_df.empty:
            return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

        reference_info = "\n".join(filtered_df['text'])
        prompt = (
            f"ì§ˆë¬¸: {question}\n"
            f"ëŒ€ë‹µí•´ì¤„ ë•Œ ì—…ì¢…ë³„ë¡œ ê°€ëŠ¥í•˜ë©´ í•˜ë‚˜ì”© ì¶”ì²œí•´ì¤˜. "
            f"ê·¸ë¦¬ê³  ì¶”ê°€ì ìœ¼ë¡œ ì˜¤ë˜ëœ ë§›ì§‘ê³¼ ìƒˆë¡œìš´ ë§›ì§‘ì„ ê°ê° ì¶”ì²œí•´ì¤˜.\n"
            f"ì°¸ê³ í•  ì •ë³´: {reference_info}\nì‘ë‹µ:"
        )
        response = model.generate_content(prompt)
        
        # ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = response.candidates[0].content.parts[0].text
        return extracted_text
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# **8. ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ë¡œë“œ ê¸°ëŠ¥**
history_path = os.path.join(module_path, 'conversation_history.json')

def save_conversation_history(conversations):
    try:
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(conversations, f, ensure_ascii=False, indent=4)
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ëŒ€í™” ê¸°ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def load_conversation_history():
    if os.path.exists(history_path):
        try:
            with open(history_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            st.error(f"ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return []
    return []

def initialize_conversation(conv_id=None):
    # conv_idê°€ ì œê³µë˜ë©´ "ëŒ€í™” ì„¸ì…˜ {conv_id}"ë¡œ ì œëª© ì„¤ì •
    title = f"ëŒ€í™” ì„¸ì…˜ {conv_id}" if conv_id is not None else ""
    return {
        "id": conv_id,  # UUID ëŒ€ì‹  ìˆ«ì ID ì‚¬ìš©
        "title": title,
        "messages": []
    }

# **10. ëŒ€í™” ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”**
if "conversations" not in st.session_state:
    st.session_state.conversations = load_conversation_history()
    if not st.session_state.conversations:
        # ëŒ€í™” ì„¸ì…˜ì„ 1ë¡œ ì„¤ì •
        initial_conversation = initialize_conversation(1)
        st.session_state.conversations.append(initial_conversation)
    else:
        # ê¸°ì¡´ ëŒ€í™” ì„¸ì…˜ ìˆ˜ì— ë”°ë¼ ìƒˆ ì„¸ì…˜ ì œëª© ì„¤ì •
        next_id = len(st.session_state.conversations) + 1
        initial_conversation = initialize_conversation(next_id)
        st.session_state.conversations.append(initial_conversation)
    st.session_state.current_conversation = st.session_state.conversations[-1]

# **11. ì‚¬ì´ë“œë°” ìœ ì§€ ë° ëŒ€í™” ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€**
with st.sidebar:
    st.header("ğŸ’¬ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬")
    
    # ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘ ë²„íŠ¼
    if st.sidebar.button("ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"):
        # ë‹¤ìŒ ì„¸ì…˜ IDë¥¼ ì„¤ì •
        next_id = len(st.session_state.conversations) + 1
        new_conversation = initialize_conversation(next_id)
        st.session_state.conversations.append(new_conversation)
        st.session_state.current_conversation = new_conversation
    
    # ëŒ€í™” ì„¸ì…˜ ì„ íƒ
    if st.session_state.conversations:
        conversation_titles = [f"{conv['title']}" for conv in st.session_state.conversations]
        selected_conversation_title = st.sidebar.selectbox("ëŒ€í™” ì„¸ì…˜ ì„ íƒ", conversation_titles)
    
        # ì„ íƒëœ ëŒ€í™” ì„¸ì…˜ ë¡œë“œ
        selected_conversation = next(
            (conv for conv in st.session_state.conversations if conv['title'] == selected_conversation_title), 
            None
        )
    
        if selected_conversation:
            st.session_state.current_conversation = selected_conversation
        else:
            st.error("ì„ íƒí•œ ëŒ€í™” ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì±„íŒ… ë‚´ì—­ ì´ˆê¸°í™” ë²„íŠ¼
    def clear_chat_history():
        if st.session_state.current_conversation:
            st.session_state.current_conversation["messages"] = []
            # ì´ˆê¸° ì œëª©ìœ¼ë¡œ "ëŒ€í™” ì„¸ì…˜ {id}"ë¡œ ì¬ì„¤ì •
            st.session_state.current_conversation["title"] = f"ëŒ€í™” ì„¸ì…˜ {st.session_state.current_conversation['id']}"
        st.success("ì±„íŒ… ë‚´ì—­ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)
    
    # ëŒ€í™” ì €ì¥ ë²„íŠ¼
    if st.sidebar.button("ëŒ€í™” ì €ì¥"):
        save_conversation_history(st.session_state.conversations)

# **12. ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ**
for message in st.session_state.current_conversation["messages"]:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# **13. ì±„íŒ… ì…ë ¥ ë° ì‘ë‹µ ì²˜ë¦¬**
chat_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if chat_input:
    current_conv = st.session_state.current_conversation

    # ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì¼ ê²½ìš°, ì œëª© ì„¤ì • (ì§ˆë¬¸ ë‚´ìš©ìœ¼ë¡œ ë³€ê²½)
    if current_conv["title"].startswith("ëŒ€í™” ì„¸ì…˜") and len(current_conv["messages"]) == 0:
        current_conv["title"] = (chat_input[:15] + "...") if len(chat_input) > 15 else chat_input

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    user_message = {"role": "user", "content": chat_input, "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    current_conv["messages"].append(user_message)
    st.markdown(f"**ì‚¬ìš©ì:** {chat_input}")

    # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    response = generate_response_with_faiss(chat_input, df, embeddings, faiss_index, model, embed_text)
    assistant_message = {"role": "assistant", "content": response, "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    current_conv["messages"].append(assistant_message)
    st.markdown(f"**ì–´ì‹œìŠ¤í„´íŠ¸:** {response}")

    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    save_conversation_history(st.session_state.conversations)